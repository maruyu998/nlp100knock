{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "news = pd.read_csv('../Chapter06/newsCorpora.csv', sep=\"\\t\", header=None, names=[\"ID\",\"TITLE\",\"URL\",\"PUBLISHER\",\"CATEGORY\",\"STORY\",\"HOSTNAME\",\"TIMESTAMP\"],index_col=\"ID\")\n",
    "news = news.query('PUBLISHER in [\"Reuters\",\"Huffington Post\",\"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v = KeyedVectors.load_word2vec_format('../Chapter07/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def genword(title):\n",
    "    for word in nlp(title): yield word.lemma_\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "def wordparse(word):\n",
    "    if word in w2v: return word\n",
    "    if stem(word) in w2v: return stem(word)\n",
    "    if word.strip('-') in w2v: return word.strip('-')\n",
    "    if len(words := word.split('-'))>1: return \" \".join([wordparse(word) for word in words])\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "news_X = news['TITLE'].apply(lambda title:\" \".join([wordparse(word) for word in genword(title)])).apply(lambda title:re.sub(r'\\s{2,}',r' ',title))\n",
    "news_Y = news['CATEGORY'].apply(lambda cate:'btem'.find(cate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dic = {}\n",
    "for title in news_X:\n",
    "    for word in nlp(title):\n",
    "        if not word.like_num: continue\n",
    "        num_dic.setdefault(word.lemma_,0)\n",
    "        num_dic[word.lemma_] += 1\n",
    "num_set = set(map(lambda x:x[0], filter(lambda x:x[1]>10, num_dic.items()))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_X = news_X.apply(lambda t:\" \".join([w.lemma_ if not w.like_num or w in num_set else \"0\" for w in nlp(t)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_valid_X, train_X, test_valid_Y, train_Y = train_test_split(news_X, news_Y, test_size=0.8, random_state=0)\n",
    "test_X, valid_X, test_Y, valid_Y = train_test_split(test_valid_X, test_valid_Y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"train\",\"test\",\"valid\"]:\n",
    "    with open(f\"{mode}_X.feature.txt\",\"w\") as f:\n",
    "        for row in eval(f\"{mode}_X\"): f.write(row+\"\\n\")\n",
    "    with open(f\"{mode}_Y.txt\",\"w\") as f:\n",
    "        for row in eval(f\"{mode}_Y\"): f.write(str(row)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
