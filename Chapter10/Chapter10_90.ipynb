{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. データの準備\n",
    "<p>機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    !ls kftt-data-1.0* > /dev/null 2>&1 || wget -q http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
    "    !ls kftt-data-1.0.tar.gz > /dev/null 2>&1 && tar -xzf kftt-data-1.0.tar.gz && rm kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    !pip install -U ginza -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from spacy.lang.en import English\n",
    "    import spacy\n",
    "    en_nlp = English()\n",
    "    ja_nlp = spacy.load('ja_ginza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    root = \"kftt-data-1.0/data/\"\n",
    "    orig = root + \"orig/\"\n",
    "    tok2 = root + \"tok2/\"\n",
    "    import os;\n",
    "    if not os.path.exists(tok2):\n",
    "        os.makedirs(tok2)\n",
    "        for lang in [\"ja\",\"en\"]:\n",
    "            if lang == \"ja\": nlp = ja_nlp\n",
    "            if lang == \"en\": nlp = en_nlp\n",
    "            for mode in [\"train\",\"dev\",\"test\"]:\n",
    "                with open(f\"{orig}kyoto-{mode}.{lang}\") as rf, open(f\"{tok2}kyoto-{mode}.{lang}\", \"w\") as wf:\n",
    "                    for line in rf:\n",
    "                        line = line.rstrip()\n",
    "                        wf.write(\" \".join([str(w) for w in nlp(line)])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataAnalysis:\n",
    "    def __init__(self, lang=\"en\", modes:list=[\"train\",\"dev\"], prepath=\"kftt-data-1.0/data/tok2/\"):\n",
    "        self.paths = {\n",
    "            lang: { mode: f\"{prepath}kyoto-{mode}.{lang}\" for mode in [\"train\",\"dev\",\"test\"] }\n",
    "            for lang in [\"ja\",\"en\"]\n",
    "        }\n",
    "        self.set(lang, modes)\n",
    "        \n",
    "    def set(self, lang=\"en\", modes:list=[\"train\",\"dev\"]):\n",
    "        self.lang = lang\n",
    "        self.modes = modes\n",
    "        return self\n",
    "    \n",
    "    def counts(self, tups): return sum(map(lambda x:x[1], tups))\n",
    "    \n",
    "    def word_counts(self):\n",
    "        dic = {}\n",
    "        for mode in self.modes:\n",
    "            with open(self.paths[self.lang][mode]) as f:\n",
    "                for line in f:\n",
    "                    line = line.rstrip().split(' ')\n",
    "                    for elm in line:\n",
    "                        dic.setdefault(elm,0)\n",
    "                        dic[elm] += 1\n",
    "        return dic\n",
    "    \n",
    "    def length_counts(self):\n",
    "        dic = {}\n",
    "        for mode in self.modes:\n",
    "            with open(self.paths[self.lang][mode]) as f:\n",
    "                for line in f:\n",
    "                    line = line.rstrip().split(' ')\n",
    "                    dic.setdefault(len(line),0)\n",
    "                    dic[len(line)] += 1\n",
    "        return dic        \n",
    "    \n",
    "    def show_thr_list(self, dic, comment, thrs:list):\n",
    "        print(self.lang, self.modes)\n",
    "        lis = sorted(dic.items(),key=lambda x:x[1],reverse=True)\n",
    "        thrs = sorted(thrs)\n",
    "        for thr in thrs:\n",
    "            left, _ = self.split_list_by_thr(lis, thr/100)\n",
    "            print(\" {:>3}%: {}:{}\".format(thr,comment,len(left)))\n",
    "    \n",
    "    def show_word_coverage(self):\n",
    "        thrs = list(range(60,100,10)) + list(range(91,100,1)) + [100]\n",
    "        self.show_thr_list(self.word_counts(), \"dicsize\", thrs)\n",
    "    \n",
    "    def show_sentence_lengths(self):\n",
    "        thrs = list(range(70,100,20)) + list(range(91,100,2)) + [i/10 for i in range(991,1000,1)] + [100]\n",
    "        self.show_thr_list(self.length_counts(), \"datasize\", thrs)\n",
    "        \n",
    "    def split_list_by_thr(self, lis, thr, sums=None):\n",
    "        if len(lis) <= 1: return lis, []\n",
    "        if sums is None: sums = self.counts(lis)\n",
    "        left, right = lis[:len(lis)//2], lis[len(lis)//2:]\n",
    "        l_cov, r_cov = self.counts(left)/sums, self.counts(right)/sums\n",
    "        if l_cov < thr:\n",
    "            l,right = self.split_list_by_thr(right, thr-l_cov, sums)\n",
    "            left = left+l\n",
    "        else:\n",
    "            left,r = self.split_list_by_thr(left, thr, sums)\n",
    "            right = r+right\n",
    "        return left, right\n",
    "    \n",
    "    def create_dictionary(self, thr, count=True, start=100, others={}):\n",
    "        lis = sorted(self.word_counts().items(),key=lambda x:x[1],reverse=True)\n",
    "        lis = [l for l in lis if l[0] not in others.keys()]\n",
    "        left, right = self.split_list_by_thr(lis, thr)\n",
    "        if count : return {k:v for k,v in left}\n",
    "        dic = {k:v for k,v in others.items()}\n",
    "        for i,(word,_) in enumerate(left,start): dic[word] = i\n",
    "        return dic\n",
    "    \n",
    "import os\n",
    "class DataAnalysisWide(DataAnalysis):\n",
    "    def __init__(self, prepath=\"kftt-data-1.0/data/tok2/\"):\n",
    "        prepath = prepath.rstrip('/')\n",
    "        files = os.listdir(prepath)\n",
    "        langs = set([f.split(\".\")[-1] for f in files])\n",
    "        modes = set([\".\".join(f.split(\".\")[:-1]) for f in files])\n",
    "        self.paths = {lang: {mode: f\"{prepath}/{mode}.{lang}\" for mode in modes} for lang in langs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    da = DataAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja ['train']\n",
      "  60%: dicsize:176\n",
      "  70%: dicsize:792\n",
      "  80%: dicsize:2849\n",
      "  90%: dicsize:10275\n",
      "  91%: dicsize:11913\n",
      "  92%: dicsize:13911\n",
      "  93%: dicsize:16394\n",
      "  94%: dicsize:19553\n",
      "  95%: dicsize:23667\n",
      "  96%: dicsize:29224\n",
      "  97%: dicsize:37302\n",
      "  98%: dicsize:50167\n",
      "  99%: dicsize:75420\n",
      " 100%: dicsize:154658\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    da.set(\"ja\",[\"train\"]).show_word_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23667"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ja_dic = da.create_dictionary(0.95)\n",
    "    len(ja_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en ['train']\n",
      "  60%: dicsize:260\n",
      "  70%: dicsize:794\n",
      "  80%: dicsize:2365\n",
      "  90%: dicsize:8305\n",
      "  91%: dicsize:9686\n",
      "  92%: dicsize:11402\n",
      "  93%: dicsize:13570\n",
      "  94%: dicsize:16360\n",
      "  95%: dicsize:20076\n",
      "  96%: dicsize:25327\n",
      "  97%: dicsize:33352\n",
      "  98%: dicsize:47353\n",
      "  99%: dicsize:79852\n",
      " 100%: dicsize:181128\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    da.set(\"en\",[\"train\"]).show_word_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    en_dic = da.create_dictionary(0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20076"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    len(en_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    escapes = {\"[PAD]\":0, \"[MASK]\":1, \"[BOS]\":10, \"[EOS]\":11, \"[UNK]\":12}\n",
    "    ja_dic = da.set(\"ja\",[\"train\"]).create_dictionary(0.95, count=False, start=100, others=escapes)\n",
    "    en_dic = da.set(\"en\",[\"train\"]).create_dictionary(0.95, count=False, start=100, others=escapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[MASK]', 1),\n",
       " ('[BOS]', 10),\n",
       " ('[EOS]', 11),\n",
       " ('[UNK]', 12),\n",
       " ('の', 100),\n",
       " ('、', 101),\n",
       " ('に', 102),\n",
       " ('。', 103),\n",
       " ('は', 104)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sorted(ja_dic.items(), key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[MASK]', 1),\n",
       " ('[BOS]', 10),\n",
       " ('[EOS]', 11),\n",
       " ('[UNK]', 12),\n",
       " ('the', 100),\n",
       " (',', 101),\n",
       " ('of', 102),\n",
       " ('.', 103),\n",
       " ('and', 104)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sorted(en_dic.items(), key=lambda x:x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import pickle\n",
    "    with open('model_logs/ja_dic.pickle', 'wb') as f: pickle.dump(ja_dic, f)\n",
    "    with open('model_logs/en_dic.pickle', 'wb') as f: pickle.dump(en_dic, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    idpath = \"kftt-data-1.0/data/ids/\"\n",
    "    if not os.path.exists(idpath):\n",
    "        os.makedirs(idpath, exist_ok=True)\n",
    "        for lang, v in da.paths.items():\n",
    "            if lang == \"ja\": dic = ja_dic\n",
    "            if lang == \"en\": dic = en_dic\n",
    "            for mode, path in v.items():\n",
    "                with open(path) as rf, open(f\"{idpath}kyoto-{mode}.{lang}\", \"w\") as wf:\n",
    "                    for line in rf:\n",
    "                        line = line.rstrip().split(\" \")\n",
    "                        line = [\"[BOS]\"]+[w if w in dic else \"[UNK]\" for w in line]+[\"[EOS]\"]\n",
    "                        wf.write(\" \".join([str(dic[w]) for w in line]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ja ['train']\n",
      "  70%: datasize:31\n",
      "  90%: datasize:48\n",
      "  91%: datasize:50\n",
      "  93%: datasize:53\n",
      "  95%: datasize:58\n",
      "  97%: datasize:65\n",
      "  99%: datasize:82\n",
      " 99.1%: datasize:84\n",
      " 99.2%: datasize:85\n",
      " 99.3%: datasize:88\n",
      " 99.4%: datasize:90\n",
      " 99.5%: datasize:93\n",
      " 99.6%: datasize:97\n",
      " 99.7%: datasize:101\n",
      " 99.8%: datasize:109\n",
      " 99.9%: datasize:121\n",
      " 100%: datasize:209\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    da.set(\"ja\",[\"train\"]).show_sentence_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en ['train']\n",
      "  70%: datasize:33\n",
      "  90%: datasize:52\n",
      "  91%: datasize:54\n",
      "  93%: datasize:58\n",
      "  95%: datasize:64\n",
      "  97%: datasize:73\n",
      "  99%: datasize:93\n",
      " 99.1%: datasize:95\n",
      " 99.2%: datasize:97\n",
      " 99.3%: datasize:99\n",
      " 99.4%: datasize:102\n",
      " 99.5%: datasize:106\n",
      " 99.6%: datasize:111\n",
      " 99.7%: datasize:117\n",
      " 99.8%: datasize:125\n",
      " 99.9%: datasize:141\n",
      " 100%: datasize:249\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    da.set(\"en\",[\"train\"]).show_sentence_lengths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Chapter10_90.ipynb to python\n",
      "[NbConvertApp] Writing 6979 bytes to Chapter10_90.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    !jupyter nbconvert --to python Chapter10_90.ipynb\n",
    "    !cp Chapter10_90.py DataAnalysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 戻る\n",
    "[Chapter10.ipynb](./Chapter10.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
